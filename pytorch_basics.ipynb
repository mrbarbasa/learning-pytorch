{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics\n",
    "\n",
    "Following along with these tutorials:\n",
    "- https://github.com/udacity/DL_PyTorch\n",
    "- https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1178331f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7576,  0.2793],\n",
       "        [ 0.4031,  0.7347],\n",
       "        [ 0.0293,  0.7999]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.],\n",
       "        [ 1.,  1.],\n",
       "        [ 1.,  1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(x.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7576,  1.2793],\n",
       "        [ 1.4031,  1.7347],\n",
       "        [ 1.0293,  1.7999]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7576,  1.2793])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2793],\n",
       "        [ 1.7347],\n",
       "        [ 1.7999]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors typically have two forms of methods, one method that returns another tensor and another method that performs the operation in place. That is, the values in memory for that tensor are changed without creating a new tensor. In-place functions are always followed by an underscore, for example `z.add()` and `z.add_()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7576,  2.2793],\n",
       "        [ 2.4031,  2.7347],\n",
       "        [ 2.0293,  2.7999]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a new tensor z + 1\n",
    "z.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7576,  1.2793],\n",
       "        [ 1.4031,  1.7347],\n",
       "        [ 1.0293,  1.7999]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z tensor is unchanged\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7576,  2.2793],\n",
       "        [ 2.4031,  2.7347],\n",
       "        [ 2.0293,  2.7999]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 1 and update z tensor in-place\n",
    "z.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7576,  2.2793],\n",
       "        [ 2.4031,  2.7347],\n",
       "        [ 2.0293,  2.7999]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z has been updated\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.])\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.]])\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.],\n",
      "         [ 7.,  8.]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor([ 1.,  2.,  3.])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 3.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],\n",
      "         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],\n",
      "         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],\n",
      "         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],\n",
      "\n",
      "        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n",
      "         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],\n",
      "         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],\n",
      "         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],\n",
      "\n",
      "        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],\n",
      "         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],\n",
      "         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],\n",
      "         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])\n"
     ]
    }
   ],
   "source": [
    "# Generate a tensor with random data\n",
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  7.,  9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8029,  0.2366,  0.2857,  0.6898, -0.6331],\n",
      "        [ 0.8795, -0.6842,  0.4533,  0.2912, -0.8317],\n",
      "        [-0.5525,  0.6355, -0.3968, -0.6571, -1.6428],\n",
      "        [ 0.9803, -0.0421, -0.8206,  0.3133, -1.1352],\n",
      "        [ 0.3773, -0.2824, -2.5667, -1.4303,  0.5009]])\n",
      "tensor([[ 0.5438, -0.4057,  1.1341, -0.1473,  0.6272,  1.0935,  0.0939,\n",
      "          1.2381],\n",
      "        [-1.1115,  0.3501, -0.7703, -1.3459,  0.5119, -0.6933, -0.1668,\n",
      "         -0.9999]])\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200],\n",
      "         [-0.6240, -0.9773,  0.8748,  0.9873],\n",
      "         [-0.0594, -2.4919,  0.2423,  0.2883]],\n",
      "\n",
      "        [[-0.1095,  0.3126,  1.5038,  0.5038],\n",
      "         [ 0.6223, -0.4481, -0.2856,  0.3880],\n",
      "         [-1.1435, -0.6512, -0.1032,  0.6937]]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,\n",
      "          0.9873, -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,\n",
      "          0.3880, -1.1435, -0.6512, -0.1032,  0.6937]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,\n",
      "          0.9873, -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,\n",
      "          0.3880, -1.1435, -0.6512, -0.1032,  0.6937]])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping tensors\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7576,  2.2793,  2.4031],\n",
       "        [ 2.7347,  2.0293,  2.7999]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.resize_(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7576,  2.2793,  2.4031],\n",
       "        [ 2.7347,  2.0293,  2.7999]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and Back\n",
    "\n",
    "Converting between Numpy arrays and Torch tensors is super simple and useful. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80723677, 0.02946084, 0.18012851],\n",
       "       [0.31984501, 0.86435952, 0.92777395],\n",
       "       [0.2881922 , 0.5860244 , 0.17808662],\n",
       "       [0.25321784, 0.63862476, 0.98670205]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8072,  0.0295,  0.1801],\n",
       "        [ 0.3198,  0.8644,  0.9278],\n",
       "        [ 0.2882,  0.5860,  0.1781],\n",
       "        [ 0.2532,  0.6386,  0.9867]], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80723677, 0.02946084, 0.18012851],\n",
       "       [0.31984501, 0.86435952, 0.92777395],\n",
       "       [0.2881922 , 0.5860244 , 0.17808662],\n",
       "       [0.25321784, 0.63862476, 0.98670205]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6145,  0.0589,  0.3603],\n",
       "        [ 0.6397,  1.7287,  1.8555],\n",
       "        [ 0.5764,  1.1720,  0.3562],\n",
       "        [ 0.5064,  1.2772,  1.9734]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.61447355, 0.05892167, 0.36025701],\n",
       "       [0.63969001, 1.72871903, 1.8555479 ],\n",
       "       [0.57638441, 1.1720488 , 0.35617324],\n",
       "       [0.50643569, 1.27724952, 1.9734041 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs and Automatic Differentiation\n",
    "\n",
    "The concept of a computation graph is essential to efficient deep learning programming, because it allows you to not have to write the back propagation gradients yourself. A computation graph is simply a specification of how your data is combined to give you the output. Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  7.,  9.])\n",
      "<AddBackward1 object at 0x117e29550>\n"
     ]
    }
   ],
   "source": [
    "# Tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n",
    "# With requires_grad=True, you can still do all the operations you previously\n",
    "# could\n",
    "y = torch.tensor([4., 5., 6.], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.)\n",
      "<SumBackward0 object at 0x117e02cf8>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have Pytorch compute the gradient, and see that we were right: (note if you run this block multiple times, the gradient will increment. That is because Pytorch accumulates the gradient into the .grad property, since for many models this is very convenient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# Calling .backward() on any variable will run backprop, starting from it\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what is going on in the block below is crucial for being a successful programmer in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "None\n",
      "<AddBackward1 object at 0x117e2cd68>\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user-created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)\n",
    "\n",
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "# flag in-place. The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)\n",
    "\n",
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()\n",
    "\n",
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(new_z.grad_fn)\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
    "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
    "# about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensors with `.requires_grad=True` by wrapping the code block in `with torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quadcop",
   "language": "python",
   "name": "quadcop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
